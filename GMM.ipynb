{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cb0971",
   "metadata": {},
   "source": [
    "# GMMによるクラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84598b42",
   "metadata": {},
   "source": [
    "https://datachemeng.com/gaussianmixturemodel/ \n",
    "\n",
    "↑わかりやすいです"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5b8aa",
   "metadata": {},
   "source": [
    "実は、K-means法は特徴量間での相関が全くなく、分散が固定であるという仮定の下で行われるGMMといえる。\n",
    "より一般化されたK-means法だと思えばよさそう。よって非階層型クラスタリングである。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350f091",
   "metadata": {},
   "source": [
    "## クラスタリングは何のため?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddfb20",
   "metadata": {},
   "source": [
    "クラスタリングは教師なし学習あるといえる。データだけが与えられたがその中身がよくわからない、となった時に利用できる。例えば次が利用対象である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa668f",
   "metadata": {},
   "source": [
    "### パターンの発見\n",
    "\n",
    "データ内の構造やパターンを発見できる。例えば沢山の顧客データがあった時、顧客のクラスターを見つけることができればそれぞれに適したマーケティング戦略が得られるかもしれない。\n",
    "\n",
    "### 次元削減\n",
    "\n",
    "高次元のデータが与えられたとき、クラスタリングを行い、各クラスター間の重心距離が大きいものや、クラスターの中での分散が小さな値は支配的なパラメータとみなすことができる。\n",
    "\n",
    "**→でも、高次元のデータならそもそも最初のクラスタリングが難しくない? そのためにはどの特徴量に着目すればいいの?**\n",
    "\n",
    "**→分散が小さいものを除外したり、主成分分析をしたりといろいろあるらしい　今回は学習の範囲に入れませんが．．．．．．**\n",
    "\n",
    "### 異常検知\n",
    "\n",
    "どのクラスターからも外れた値を異常値として検出することができる。製造業などの分野で利用される。\n",
    "\n",
    "### 機械学習の前処理\n",
    "\n",
    "画像認識などのタスクにおいて、ラベル付けに利用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b02d5",
   "metadata": {},
   "source": [
    "## そもそもGMM（混合ガウスモデル）ってなんだ?\n",
    "\n",
    "**複数の正規分布の重ね合わせ**でデータセットの確率密度を推定するモデル(言葉の定義ががばがばです。多分間違っています)\n",
    "\n",
    "各データ点について、どの正規分布に所属するのか確率密度の高さで求めることができるので、GMMによってクラスタリングできる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ba4aa",
   "metadata": {},
   "source": [
    "## 何がわかればいいの?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19e7c8",
   "metadata": {},
   "source": [
    "コンポーネント数（混合された正規分布の数）が決まっていたとしたら、\n",
    "\n",
    "- **各コンポーネントの平均ベクトル:** $\\boldsymbol{\\mu}$\n",
    "- **各コンポーネント間の分散共分散行列:** $\\boldsymbol{\\Sigma}$\n",
    "- **混合係数ベクトル(各コンポーネントの重み):** $\\boldsymbol{\\pi}$\n",
    "\n",
    "がわかればモデルを構築することができる。これらのモデルパラメータを最尤推定法で求める必要がある。\n",
    "\n",
    "最尤推定というのは、パラメータが与えられたデータに最もフィットするように最適化することだと認識している。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55058645",
   "metadata": {},
   "source": [
    "## どうやって最尤推定なんてするの? 目的関数はどうするの?\n",
    "\n",
    "### ＞どうやって最尤推定なんてするの?\n",
    "**EMアルゴリズム**を使用することが多いらしい\n",
    "\n",
    "**EMアルゴリズム**というのは　EvaluateメソッドとModifyメソッドを使用してそれぞれ尤度評価、パラメータ更新を何度も繰り返して行うアルゴリズム。\n",
    "\n",
    "### ＞目的関数はどうするの?\n",
    "**対数尤度関数**を使用することが多いらしい。次式で与えられる。\n",
    "\n",
    "$N$：データ点数,  $n$：混合成分数,  $N(x_j | \\mu_k, \\Sigma_k)$：第$k$成分のガウス分布の確率密度関数\n",
    "\n",
    "$\\log L(\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} | \\mathbf{X}) = \\log \\left\\{ \\prod_{j=1}^{N} \\sum_{k=1}^{n} \\pi_k N(x_j | \\mu_k, \\Sigma_k) \\right\\} = \\sum_{j=1}^{N} \\log \\left\\{ \\sum_{k=1}^{n} \\pi_k N(x_j | \\mu_k, \\Sigma_k) \\right\\}$\n",
    "\n",
    "この値を最大にするように最適化を行う。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb3056",
   "metadata": {},
   "source": [
    "## 実際にやってみよう！！\n",
    "\n",
    "式をちゃんと実装しています。しかし、scikit-learnというライブラリを使用すれば、2行でこれらの演算を行うことができます。\n",
    "\n",
    "また、データセットは複数種類のアヤメの花のデータセットを使用しています。がくと花びらの幅と長さでどの品種かを推定できるらしいです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e7e29",
   "metadata": {},
   "source": [
    "### ライブラリインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99870e4b",
   "metadata": {},
   "source": [
    "### クラス、メソッドの作製"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5673aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMFromScratch:\n",
    "    def __init__(self, n_components=3, max_iter=10, tol=1e-3, random_state=None):\n",
    "        \"\"\"\n",
    "        Gaussian Mixture Model implementation from scratch using EM algorithm\n",
    "        \n",
    "        Parameters:\n",
    "        - n_components: Number of Gaussian components\n",
    "        - max_iter: Maximum number of iterations\n",
    "        - tol: Tolerance for convergence\n",
    "        - random_state: Random seed for reproducibility\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Parameters to be learned\n",
    "        self.means_ = None          # Mean vectors for each component\n",
    "        self.covariances_ = None    # Covariance matrices for each component\n",
    "        self.weights_ = None        # Mixing coefficients (prior probabilities)\n",
    "        self.responsibilities_ = None  # Posterior probabilities\n",
    "        self.prev_means_ = None  # Previous means for convergence check\n",
    "        self.prev_covariances_ = None  # Previous covariances for convergence check \n",
    "        self.prev_weights_ = None  # Previous weights for convergence check\n",
    "        self.log_likelihood_history = []  # Track convergence\n",
    "        \n",
    "    def _initialize_parameters(self, X):\n",
    "        \"\"\"\n",
    "        Initialize parameters for EM algorithm\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        # Initialize means randomly from data points\n",
    "        random_indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "        self.means_ = X[random_indices].copy()\n",
    "        \n",
    "        # Initialize covariances as identity matrices scaled by data variance\n",
    "        data_variance = np.var(X, axis=0)\n",
    "        self.covariances_ = np.array([np.diag(data_variance) for _ in range(self.n_components)])\n",
    "        \n",
    "        # Initialize weights uniformly\n",
    "        self.weights_ = np.ones(self.n_components) / self.n_components\n",
    "        \n",
    "        print(\"Initial parameters:\")\n",
    "        print(f\"Means: {self.means_}\")\n",
    "        print(f\"Weights: {self.weights_}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    def _e_step(self, X):\n",
    "        \"\"\"\n",
    "        E-step: Calculate responsibilities (posterior probabilities)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        self.responsibilities_ = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        # Calculate likelihood for each component\n",
    "        for k in range(self.n_components):\n",
    "            likelihood = multivariate_normal.pdf(X, self.means_[k], self.covariances_[k])\n",
    "            self.responsibilities_[:, k] = self.weights_[k] * likelihood\n",
    "        \n",
    "        # likelihood before normalization\n",
    "        total_likelihood = np.sum(self.responsibilities_, axis=1, keepdims=True)\n",
    "        \n",
    "        # Normalize to get posterior probabilities\n",
    "        self.responsibilities_ /= total_likelihood\n",
    "        \n",
    "        # calculalte log likelihood\n",
    "        log_likelihood = np.sum(np.log(total_likelihood.flatten()))\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def _m_step(self, X):\n",
    "        \"\"\"\n",
    "        M-step: Update parameters based on responsibilities\n",
    "        \n",
    "        Update formulas:\n",
    "        N_k = Σ_i r_ik\n",
    "        μ_k = (Σ_i r_ik * x_i) / N_k\n",
    "        Σ_k = (Σ_i r_ik * (x_i - μ_k)(x_i - μ_k)^T) / N_k\n",
    "        π_k = N_k / N\n",
    "        \"\"\"\n",
    "        # save previous parameters for convergence check\n",
    "        self.prev_means_ = self.means_\n",
    "        self.prev_covariances_ = self.covariances_\n",
    "        self.prev_weights_ = self.weights_\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Calculate effective number of points for each component\n",
    "        N_k = np.sum(self.responsibilities_, axis=0)\n",
    "        \n",
    "        # Update weights (mixing coefficients)\n",
    "        self.weights_ = N_k / n_samples\n",
    "        \n",
    "        # Update means\n",
    "        for k in range(self.n_components):\n",
    "            if N_k[k] > 1e-10:  # Avoid division by zero\n",
    "                self.means_[k] = np.sum(self.responsibilities_[:, k:k+1] * X, axis=0) / N_k[k]\n",
    "        \n",
    "        # Update covariances\n",
    "        for k in range(self.n_components):\n",
    "            if N_k[k] > 1e-10:  # Avoid division by zero\n",
    "                diff = X - self.means_[k]\n",
    "                weighted_diff = self.responsibilities_[:, k:k+1] * diff\n",
    "                self.covariances_[k] = np.dot(weighted_diff.T, diff) / N_k[k]\n",
    "                \n",
    "                # Add small regularization to avoid singular matrices\n",
    "                self.covariances_[k] += np.eye(n_features) * 1e-6\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the GMM model using EM algorithm\n",
    "        \"\"\"\n",
    "        print(\"=== GMM Learning Process ===\")\n",
    "        print(f\"Data shape: {X.shape}\")\n",
    "        print(f\"Number of components: {self.n_components}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X)\n",
    "        log_likelihood = self._e_step(X)\n",
    "        # Initialize log-likelihood for convergence check\n",
    "        prev_log_likelihood = -np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "\n",
    "            # M-step\n",
    "            self._m_step(X)\n",
    "\n",
    "            # E-step\n",
    "            log_likelihood = self._e_step(X)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if abs(log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "            \n",
    "            # Print progress every 10 iterations\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                print(f\"Iteration {iteration + 1}: Log-likelihood = {log_likelihood:.4f}\")\n",
    "            \n",
    "            self.log_likelihood_history.append(log_likelihood)\n",
    "            prev_log_likelihood = log_likelihood\n",
    "        \n",
    "        print(f\"Final log-likelihood: {log_likelihood:.4f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster labels for new data\n",
    "        \"\"\"\n",
    "        self._e_step(X)\n",
    "        return np.argmax(self.responsibilities_, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict posterior probabilities for new data\n",
    "        \"\"\"\n",
    "        self._e_step(X)\n",
    "        return self.responsibilities_\n",
    "    \n",
    "    def score_samples(self, X):\n",
    "        \"\"\"\n",
    "        Calculate log probability density for each sample\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        log_prob = np.zeros(n_samples)\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            likelihood = multivariate_normal.pdf(X, self.means_[k], self.covariances_[k])\n",
    "            log_prob += self.weights_[k] * likelihood\n",
    "        \n",
    "        return np.log(log_prob)\n",
    "    \n",
    "    def fit_multi_init(self, X, n_init=1):\n",
    "        \"\"\"\n",
    "        Fit GMM with multiple random initializations and choose the best result.\n",
    "        \"\"\"\n",
    "        best_log_likelihood = -np.inf\n",
    "        best_params = None\n",
    "        best_history = None\n",
    "\n",
    "        for seed in range(n_init):\n",
    "            self.random_state = seed\n",
    "            self._initialize_parameters(X)\n",
    "            self.log_likelihood_history = []\n",
    "            prev_log_likelihood = -np.inf\n",
    "            log_likelihood = self._e_step(X)\n",
    "\n",
    "            for iteration in range(self.max_iter):\n",
    "                self._m_step(X)\n",
    "                log_likelihood = self._e_step(X)\n",
    "                self.log_likelihood_history.append(log_likelihood)\n",
    "                if abs(log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                    break\n",
    "                prev_log_likelihood = log_likelihood\n",
    "\n",
    "            if log_likelihood > best_log_likelihood:\n",
    "                best_log_likelihood = log_likelihood\n",
    "                best_params = (self.means_.copy(), self.covariances_.copy(), self.weights_.copy())\n",
    "                best_history = self.log_likelihood_history.copy()\n",
    "\n",
    "        # Set the best parameters\n",
    "        self.means_, self.covariances_, self.weights_ = best_params\n",
    "        self.log_likelihood_history = best_history\n",
    "        print(f\"Best log-likelihood after {n_init} initializations: {best_log_likelihood:.4f}\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6b6ef",
   "metadata": {},
   "source": [
    "### 一つの初期値から始めるパターンを実行してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('Iris_dataset.csv')\n",
    "X = df[['petal_length', 'petal_width']].values #←花びら(petal)についてのみデータを読み込んでいます変えてみると発見があるかも。\n",
    "feature_names = ['petal_length', 'petal_width']\n",
    "\n",
    "# Create and train custom GMM\n",
    "print(\"Training Custom GMM...\")\n",
    "custom_gmm = GMMFromScratch(n_components=3, random_state=42, max_iter=1000) #←乱数シード値は42にされることが慣例らしいです。変えてみると結果が変わります。\n",
    "custom_gmm.fit(X)\n",
    "\n",
    "# Get predictions\n",
    "cluster_labels = custom_gmm.predict(X)\n",
    "cluster_probs = custom_gmm.predict_proba(X)\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Original data\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "species_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "df['species_num'] = df['species'].map(species_mapping)\n",
    "colors = ['red', 'blue', 'green']\n",
    "species_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "for i, (color, species) in enumerate(zip(colors, species_names)):\n",
    "    mask = df['species_num'] == i\n",
    "    ax1.scatter(X[mask, 0], X[mask, 1], c=color, alpha=0.7, label=species)\n",
    "ax1.set_xlabel(feature_names[0])\n",
    "ax1.set_ylabel(feature_names[1])\n",
    "ax1.set_title('Original Data (True Species)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Custom GMM results\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    ax2.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, label=f'Cluster {i}')\n",
    "ax2.set_xlabel(feature_names[0])\n",
    "ax2.set_ylabel(feature_names[1])\n",
    "ax2.set_title('Custom GMM Clustering Results')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning curve\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "ax3.plot(custom_gmm.log_likelihood_history, 'b-', linewidth=2)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Log-likelihood')\n",
    "ax3.set_title('Learning Curve (Convergence)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Probability density with contours\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "log_prob = custom_gmm.score_samples(grid_points)\n",
    "prob = np.exp(log_prob)\n",
    "prob = prob.reshape(xx.shape)\n",
    "\n",
    "contour = ax4.contour(xx, yy, prob, levels=10, colors='black', alpha=0.4)\n",
    "ax4.contourf(xx, yy, prob, levels=50, cmap='viridis', alpha=0.6)\n",
    "ax4.set_xlabel(feature_names[0])\n",
    "ax4.set_ylabel(feature_names[1])\n",
    "ax4.set_title('Probability Density (2D)')\n",
    "\n",
    "# 5. Covariance ellipses\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    ax5.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, label=f'Cluster {i}')\n",
    "\n",
    "# Draw ellipses for each component\n",
    "for i in range(custom_gmm.n_components):\n",
    "    mean = custom_gmm.means_[i]\n",
    "    cov = custom_gmm.covariances_[i]\n",
    "    \n",
    "    eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "    angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))\n",
    "    width = 2 * np.sqrt(eigenvals[0]) * 2\n",
    "    height = 2 * np.sqrt(eigenvals[1]) * 2\n",
    "    \n",
    "    ellipse = Ellipse(mean, width, height, angle=angle, \n",
    "                     facecolor='none', edgecolor=colors[i], \n",
    "                     linewidth=2, linestyle='--', alpha=0.8)\n",
    "    ax5.add_patch(ellipse)\n",
    "\n",
    "ax5.set_xlabel(feature_names[0])\n",
    "ax5.set_ylabel(feature_names[1])\n",
    "ax5.set_title('Clusters with Covariance Ellipses')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. 3D probability surface\n",
    "ax6 = fig.add_subplot(2, 3, 6, projection='3d')\n",
    "surf = ax6.plot_surface(xx, yy, prob, cmap='viridis', alpha=0.8)\n",
    "ax6.set_xlabel(feature_names[0])\n",
    "ax6.set_ylabel(feature_names[1])\n",
    "ax6.set_zlabel('Probability Density')\n",
    "ax6.set_title('3D Probability Surface')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\n=== Final GMM Parameters ===\")\n",
    "print(\"Learned Means:\")\n",
    "for i, mean in enumerate(custom_gmm.means_):\n",
    "    print(f\"  Component {i}: [{mean[0]:.3f}, {mean[1]:.3f}]\")\n",
    "\n",
    "print(\"\\nLearned Weights:\")\n",
    "for i, weight in enumerate(custom_gmm.weights_):\n",
    "    print(f\"  Component {i}: {weight:.3f}\")\n",
    "\n",
    "print(\"\\nLearned Covariances:\")\n",
    "for i, cov in enumerate(custom_gmm.covariances_):\n",
    "    print(f\"  Component {i}:\")\n",
    "    print(f\"    [[{cov[0,0]:.3f}, {cov[0,1]:.3f}]\")\n",
    "    print(f\"     [{cov[1,0]:.3f}, {cov[1,1]:.3f}]]\")\n",
    "\n",
    "# Compare with sklearn results\n",
    "\n",
    "print(\"\\n=== Comparison with sklearn ===\")\n",
    "print(\"Custom GMM final log-likelihood:\", custom_gmm.log_likelihood_history[-1])\n",
    "\n",
    "# Confusion matrix\n",
    "comparison = pd.crosstab(df['species'], cluster_labels, \n",
    "                        rownames=['True Species'], colnames=['Predicted Cluster'])\n",
    "print(\"\\n=== True Species vs Custom GMM Clusters ===\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c9f4e",
   "metadata": {},
   "source": [
    "### 複数の初期値から始めて良いパターンを探す方法で実行してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('Iris_dataset.csv')\n",
    "X = df[['petal_length', 'petal_width']].values\n",
    "feature_names = ['petal_length', 'petal_width']\n",
    "\n",
    "# Create and train custom GMM\n",
    "print(\"Training Custom GMM...\")\n",
    "custom_gmm = GMMFromScratch(n_components=3, random_state=42, max_iter=1000)\n",
    "custom_gmm.fit_multi_init(X, n_init=10)\n",
    "\n",
    "# Get predictions\n",
    "cluster_labels = custom_gmm.predict(X)\n",
    "cluster_probs = custom_gmm.predict_proba(X)\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Original data\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "species_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "df['species_num'] = df['species'].map(species_mapping)\n",
    "colors = ['red', 'blue', 'green']\n",
    "species_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "for i, (color, species) in enumerate(zip(colors, species_names)):\n",
    "    mask = df['species_num'] == i\n",
    "    ax1.scatter(X[mask, 0], X[mask, 1], c=color, alpha=0.7, label=species)\n",
    "ax1.set_xlabel(feature_names[0])\n",
    "ax1.set_ylabel(feature_names[1])\n",
    "ax1.set_title('Original Data (True Species)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Custom GMM results\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    ax2.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, label=f'Cluster {i}')\n",
    "ax2.set_xlabel(feature_names[0])\n",
    "ax2.set_ylabel(feature_names[1])\n",
    "ax2.set_title('Custom GMM Clustering Results')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning curve\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "ax3.plot(custom_gmm.log_likelihood_history, 'b-', linewidth=2)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Log-likelihood')\n",
    "ax3.set_title('Learning Curve (Convergence)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Probability density with contours\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "log_prob = custom_gmm.score_samples(grid_points)\n",
    "prob = np.exp(log_prob)\n",
    "prob = prob.reshape(xx.shape)\n",
    "\n",
    "contour = ax4.contour(xx, yy, prob, levels=10, colors='black', alpha=0.4)\n",
    "ax4.contourf(xx, yy, prob, levels=50, cmap='viridis', alpha=0.6)\n",
    "ax4.set_xlabel(feature_names[0])\n",
    "ax4.set_ylabel(feature_names[1])\n",
    "ax4.set_title('Probability Density (2D)')\n",
    "\n",
    "# 5. Covariance ellipses\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    ax5.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, label=f'Cluster {i}')\n",
    "\n",
    "# Draw ellipses for each component\n",
    "for i in range(custom_gmm.n_components):\n",
    "    mean = custom_gmm.means_[i]\n",
    "    cov = custom_gmm.covariances_[i]\n",
    "    \n",
    "    eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "    angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))\n",
    "    width = 2 * np.sqrt(eigenvals[0]) * 2\n",
    "    height = 2 * np.sqrt(eigenvals[1]) * 2\n",
    "    \n",
    "    ellipse = Ellipse(mean, width, height, angle=angle, \n",
    "                     facecolor='none', edgecolor=colors[i], \n",
    "                     linewidth=2, linestyle='--', alpha=0.8)\n",
    "    ax5.add_patch(ellipse)\n",
    "\n",
    "ax5.set_xlabel(feature_names[0])\n",
    "ax5.set_ylabel(feature_names[1])\n",
    "ax5.set_title('Clusters with Covariance Ellipses')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. 3D probability surface\n",
    "ax6 = fig.add_subplot(2, 3, 6, projection='3d')\n",
    "surf = ax6.plot_surface(xx, yy, prob, cmap='viridis', alpha=0.8)\n",
    "ax6.set_xlabel(feature_names[0])\n",
    "ax6.set_ylabel(feature_names[1])\n",
    "ax6.set_zlabel('Probability Density')\n",
    "ax6.set_title('3D Probability Surface')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\n=== Final GMM Parameters ===\")\n",
    "print(\"Learned Means:\")\n",
    "for i, mean in enumerate(custom_gmm.means_):\n",
    "    print(f\"  Component {i}: [{mean[0]:.3f}, {mean[1]:.3f}]\")\n",
    "\n",
    "print(\"\\nLearned Weights:\")\n",
    "for i, weight in enumerate(custom_gmm.weights_):\n",
    "    print(f\"  Component {i}: {weight:.3f}\")\n",
    "\n",
    "print(\"\\nLearned Covariances:\")\n",
    "for i, cov in enumerate(custom_gmm.covariances_):\n",
    "    print(f\"  Component {i}:\")\n",
    "    print(f\"    [[{cov[0,0]:.3f}, {cov[0,1]:.3f}]\")\n",
    "    print(f\"     [{cov[1,0]:.3f}, {cov[1,1]:.3f}]]\")\n",
    "\n",
    "# Compare with sklearn results\n",
    "\n",
    "print(\"\\n=== Comparison with sklearn ===\")\n",
    "print(\"Custom GMM final log-likelihood:\", custom_gmm.log_likelihood_history[-1])\n",
    "\n",
    "# Confusion matrix\n",
    "comparison = pd.crosstab(df['species'], cluster_labels, \n",
    "                        rownames=['True Species'], colnames=['Predicted Cluster'])\n",
    "print(\"\\n=== True Species vs Custom GMM Clusters ===\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad2ec1",
   "metadata": {},
   "source": [
    "→変な局所解を発見してしまっており、うまくいかない。データセットが小数1桁しかないのが原因か?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be36c170",
   "metadata": {},
   "source": [
    "### データセットにランダムノイズを与えたものでも試してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('Iris_dataset_noisy.csv')\n",
    "X = df[['petal_length', 'petal_width']].values\n",
    "feature_names = ['petal_length', 'petal_width']\n",
    "\n",
    "# Create and train custom GMM\n",
    "print(\"Training Custom GMM...\")\n",
    "custom_gmm = GMMFromScratch(n_components=3, random_state=42, max_iter=1000)\n",
    "#custom_gmm.fit(X)\n",
    "custom_gmm.fit_multi_init(X, n_init=10)\n",
    "\n",
    "# Get predictions\n",
    "cluster_labels = custom_gmm.predict(X)\n",
    "cluster_probs = custom_gmm.predict_proba(X)\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Original data\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "species_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "df['species_num'] = df['species'].map(species_mapping)\n",
    "colors = ['red', 'blue', 'green']\n",
    "species_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "for i, (color, species) in enumerate(zip(colors, species_names)):\n",
    "    mask = df['species_num'] == i\n",
    "    ax1.scatter(X[mask, 0], X[mask, 1], c=color, alpha=0.7, label=species)\n",
    "ax1.set_xlabel(feature_names[0])\n",
    "ax1.set_ylabel(feature_names[1])\n",
    "ax1.set_title('Original Data (True Species)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Custom GMM results\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    ax2.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, label=f'Cluster {i}')\n",
    "ax2.set_xlabel(feature_names[0])\n",
    "ax2.set_ylabel(feature_names[1])\n",
    "ax2.set_title('Custom GMM Clustering Results')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning curve\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "ax3.plot(custom_gmm.log_likelihood_history, 'b-', linewidth=2)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Log-likelihood')\n",
    "ax3.set_title('Learning Curve (Convergence)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Probability density with contours\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "log_prob = custom_gmm.score_samples(grid_points)\n",
    "prob = np.exp(log_prob)\n",
    "prob = prob.reshape(xx.shape)\n",
    "\n",
    "contour = ax4.contour(xx, yy, prob, levels=10, colors='black', alpha=0.4)\n",
    "ax4.contourf(xx, yy, prob, levels=50, cmap='viridis', alpha=0.6)\n",
    "ax4.set_xlabel(feature_names[0])\n",
    "ax4.set_ylabel(feature_names[1])\n",
    "ax4.set_title('Probability Density (2D)')\n",
    "\n",
    "# 5. Covariance ellipses\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    ax5.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, label=f'Cluster {i}')\n",
    "\n",
    "# Draw ellipses for each component\n",
    "for i in range(custom_gmm.n_components):\n",
    "    mean = custom_gmm.means_[i]\n",
    "    cov = custom_gmm.covariances_[i]\n",
    "    \n",
    "    eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "    angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))\n",
    "    width = 2 * np.sqrt(eigenvals[0]) * 2\n",
    "    height = 2 * np.sqrt(eigenvals[1]) * 2\n",
    "    \n",
    "    ellipse = Ellipse(mean, width, height, angle=angle, \n",
    "                     facecolor='none', edgecolor=colors[i], \n",
    "                     linewidth=2, linestyle='--', alpha=0.8)\n",
    "    ax5.add_patch(ellipse)\n",
    "\n",
    "ax5.set_xlabel(feature_names[0])\n",
    "ax5.set_ylabel(feature_names[1])\n",
    "ax5.set_title('Clusters with Covariance Ellipses')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. 3D probability surface\n",
    "ax6 = fig.add_subplot(2, 3, 6, projection='3d')\n",
    "surf = ax6.plot_surface(xx, yy, prob, cmap='viridis', alpha=0.8)\n",
    "ax6.set_xlabel(feature_names[0])\n",
    "ax6.set_ylabel(feature_names[1])\n",
    "ax6.set_zlabel('Probability Density')\n",
    "ax6.set_title('3D Probability Surface')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\n=== Final GMM Parameters ===\")\n",
    "print(\"Learned Means:\")\n",
    "for i, mean in enumerate(custom_gmm.means_):\n",
    "    print(f\"  Component {i}: [{mean[0]:.3f}, {mean[1]:.3f}]\")\n",
    "\n",
    "print(\"\\nLearned Weights:\")\n",
    "for i, weight in enumerate(custom_gmm.weights_):\n",
    "    print(f\"  Component {i}: {weight:.3f}\")\n",
    "\n",
    "print(\"\\nLearned Covariances:\")\n",
    "for i, cov in enumerate(custom_gmm.covariances_):\n",
    "    print(f\"  Component {i}:\")\n",
    "    print(f\"    [[{cov[0,0]:.3f}, {cov[0,1]:.3f}]\")\n",
    "    print(f\"     [{cov[1,0]:.3f}, {cov[1,1]:.3f}]]\")\n",
    "\n",
    "# Compare with sklearn results\n",
    "\n",
    "print(\"\\n=== Comparison with sklearn ===\")\n",
    "print(\"Custom GMM final log-likelihood:\", custom_gmm.log_likelihood_history[-1])\n",
    "\n",
    "# Confusion matrix\n",
    "comparison = pd.crosstab(df['species'], cluster_labels, \n",
    "                        rownames=['True Species'], colnames=['Predicted Cluster'])\n",
    "print(\"\\n=== True Species vs Custom GMM Clusters ===\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7db072",
   "metadata": {},
   "source": [
    "変なことにはならないようだ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f90c9",
   "metadata": {},
   "source": [
    "### GMMが威力を発揮するパターン"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20378b95",
   "metadata": {},
   "source": [
    "#### 対象データの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('gmm_test_data.csv')\n",
    "X = df[['X', 'Y']].values\n",
    "feature_names = ['X', 'Y']\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# 1. Original data\n",
    "species_mapping = {'center': 0, 'donut': 1}\n",
    "df['species_num'] = df['species'].map(species_mapping)\n",
    "colors = ['red', 'blue', 'green']\n",
    "species_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "for i, (color, species) in enumerate(zip(colors, species_names)):\n",
    "    mask = df['species_num'] == i\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=color, alpha=0.7, label=species)\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.title('Original Data (True Species)')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c57240",
   "metadata": {},
   "source": [
    "#### K-means法によるクラスタリングの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "idx = np.zeros(X.shape[0])\n",
    "centers = np.array([[-0.1, 0], [0.1, 0]])\n",
    "\n",
    "# WCSSを格納するリスト\n",
    "wcss_list = []\n",
    "max_iter = 1000\n",
    "centers_prev = np.zeros_like(centers)\n",
    "\n",
    "for j in range(max_iter):\n",
    "    # クラスタの割り当て\n",
    "    for i in range(X.shape[0]):\n",
    "        idx[i] = np.argmin(np.sum((X[i, :] - centers) ** 2, axis=1))\n",
    "\n",
    "    # センターの更新\n",
    "    for k in range(K):\n",
    "        if np.any(idx == k):\n",
    "            centers_prev[k, :] = centers[k, :]\n",
    "            centers[k, :] = X[idx == k, :].mean(axis=0)\n",
    "            \n",
    "\n",
    "    # 収束判定\n",
    "    if j > 0 and np.all(np.abs(centers - centers_prev) < 1e-6):\n",
    "        print(f\"Converged after {j} iterations\")\n",
    "        break\n",
    "\n",
    "    # WCSSの計算\n",
    "    wcss = 0\n",
    "    for k in range(K):\n",
    "        points = X[idx == k]\n",
    "        wcss += np.sum((points - centers[k]) ** 2)\n",
    "    wcss_list.append(wcss)\n",
    "    print(f\"Step {j}: WCSS = {wcss:.2f}\")\n",
    "\n",
    "\n",
    "# 2. K-means results\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i in range(K):\n",
    "    mask = idx == i\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, label=f'Cluster {i}')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', s=100, label='Centers')\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fd738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('gmm_test_data.csv')\n",
    "X = df[['X', 'Y']].values #←花びら(petal)についてのみデータを読み込んでいます変えてみると発見があるかも。\n",
    "feature_names = ['X', 'Y']\n",
    "\n",
    "# Create and train custom GMM\n",
    "print(\"Training Custom GMM...\")\n",
    "custom_gmm = GMMFromScratch(n_components=2, random_state=42, max_iter=1000) #←乱数シード値は42にされることが慣例らしいです。変えてみると結果が変わります。\n",
    "custom_gmm.fit_multi_init(X, n_init=100)\n",
    "\n",
    "# Get predictions\n",
    "cluster_labels = custom_gmm.predict(X)\n",
    "cluster_probs = custom_gmm.predict_proba(X)\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Original data\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "species_mapping = {'center': 0, 'donut': 1}\n",
    "df['species_num'] = df['species'].map(species_mapping)\n",
    "colors = ['red', 'blue', 'green']\n",
    "species_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "for i, (color, species) in enumerate(zip(colors, species_names)):\n",
    "    mask = df['species_num'] == i\n",
    "    ax1.scatter(X[mask, 0], X[mask, 1], c=color, alpha=0.7, label=species)\n",
    "ax1.set_xlabel(feature_names[0])\n",
    "ax1.set_ylabel(feature_names[1])\n",
    "ax1.set_title('Original Data (True Species)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Custom GMM results\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    ax2.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, label=f'Cluster {i}')\n",
    "ax2.set_xlabel(feature_names[0])\n",
    "ax2.set_ylabel(feature_names[1])\n",
    "ax2.set_title('Custom GMM Clustering Results')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning curve\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "ax3.plot(custom_gmm.log_likelihood_history, 'b-', linewidth=2)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Log-likelihood')\n",
    "ax3.set_title('Learning Curve (Convergence)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Probability density with contours\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "log_prob = custom_gmm.score_samples(grid_points)\n",
    "prob = np.exp(log_prob)\n",
    "prob = prob.reshape(xx.shape)\n",
    "\n",
    "contour = ax4.contour(xx, yy, prob, levels=10, colors='black', alpha=0.4)\n",
    "ax4.contourf(xx, yy, prob, levels=50, cmap='viridis', alpha=0.6)\n",
    "ax4.set_xlabel(feature_names[0])\n",
    "ax4.set_ylabel(feature_names[1])\n",
    "ax4.set_title('Probability Density (2D)')\n",
    "\n",
    "# 5. Covariance ellipses\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "for i in range(3):\n",
    "    mask = cluster_labels == i\n",
    "    ax5.scatter(X[mask, 0], X[mask, 1], c=colors[i], alpha=0.7, label=f'Cluster {i}')\n",
    "\n",
    "# Draw ellipses for each component\n",
    "for i in range(custom_gmm.n_components):\n",
    "    mean = custom_gmm.means_[i]\n",
    "    cov = custom_gmm.covariances_[i]\n",
    "    \n",
    "    eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "    angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))\n",
    "    width = 2 * np.sqrt(eigenvals[0]) * 2\n",
    "    height = 2 * np.sqrt(eigenvals[1]) * 2\n",
    "    \n",
    "    ellipse = Ellipse(mean, width, height, angle=angle, \n",
    "                     facecolor='none', edgecolor=colors[i], \n",
    "                     linewidth=2, linestyle='--', alpha=0.8)\n",
    "    ax5.add_patch(ellipse)\n",
    "\n",
    "ax5.set_xlabel(feature_names[0])\n",
    "ax5.set_ylabel(feature_names[1])\n",
    "ax5.set_title('Clusters with Covariance Ellipses')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. 3D probability surface\n",
    "ax6 = fig.add_subplot(2, 3, 6, projection='3d')\n",
    "surf = ax6.plot_surface(xx, yy, prob, cmap='viridis', alpha=0.8)\n",
    "ax6.set_xlabel(feature_names[0])\n",
    "ax6.set_ylabel(feature_names[1])\n",
    "ax6.set_zlabel('Probability Density')\n",
    "ax6.set_title('3D Probability Surface')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\n=== Final GMM Parameters ===\")\n",
    "print(\"Learned Means:\")\n",
    "for i, mean in enumerate(custom_gmm.means_):\n",
    "    print(f\"  Component {i}: [{mean[0]:.3f}, {mean[1]:.3f}]\")\n",
    "\n",
    "print(\"\\nLearned Weights:\")\n",
    "for i, weight in enumerate(custom_gmm.weights_):\n",
    "    print(f\"  Component {i}: {weight:.3f}\")\n",
    "\n",
    "print(\"\\nLearned Covariances:\")\n",
    "for i, cov in enumerate(custom_gmm.covariances_):\n",
    "    print(f\"  Component {i}:\")\n",
    "    print(f\"    [[{cov[0,0]:.3f}, {cov[0,1]:.3f}]\")\n",
    "    print(f\"     [{cov[1,0]:.3f}, {cov[1,1]:.3f}]]\")\n",
    "\n",
    "# Compare with sklearn results\n",
    "\n",
    "print(\"\\n=== Comparison with sklearn ===\")\n",
    "print(\"Custom GMM final log-likelihood:\", custom_gmm.log_likelihood_history[-1])\n",
    "\n",
    "# Confusion matrix\n",
    "comparison = pd.crosstab(df['species'], cluster_labels, \n",
    "                        rownames=['True Species'], colnames=['Predicted Cluster'])\n",
    "print(\"\\n=== True Species vs Custom GMM Clusters ===\")\n",
    "print(comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
